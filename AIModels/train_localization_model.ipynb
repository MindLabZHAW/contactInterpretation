{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "class create_tensor_dataset_localization(Dataset):\n",
    "    def __init__(self, path = './dataset/realData/contact_detection_train.csv', transform = transforms.Compose([transforms.ToTensor()]), num_classes =5,\n",
    "                 num_features_dataset = 14, num_features_lstm = 2, data_seq = 28, desired_seq = 28, localization= False, collision =False):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.num_features_dataset = num_features_dataset\n",
    "        self.num_features_lstm = num_features_lstm\n",
    "        self.data_seq = data_seq\n",
    "        self.desired_seq = desired_seq\n",
    "        self.dof = 7\n",
    "        self.num_classes = num_classes\n",
    "        self.localization = localization\n",
    "        self.collision = collision\n",
    "        if collision and localization:\n",
    "            print('collision and localization cannot be true at the same time!')\n",
    "            exit()\n",
    "            \n",
    "\n",
    "        self.read_dataset()\n",
    "        self.data_in_seq()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_target)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        data_sample = torch.tensor(self.data_input.iloc[idx].values)\n",
    "        data_sample = torch.reshape(data_sample, (self.dof ,self.num_features_lstm*self.desired_seq))\n",
    "\n",
    "        target = self.data_target.iloc[idx]\n",
    "\n",
    "        return data_sample, target\n",
    "\n",
    "\n",
    "    def read_dataset(self):\n",
    "        \n",
    "        # laod data from csv file\n",
    "        if self.path[(len(self.path)-3): len(self.path)] == 'csv':\n",
    "            data = pd.read_csv(self.path)\n",
    "        elif self.path[(len(self.path)-3): len(self.path)] == 'pkl':\n",
    "            data = pd.read_pickle(self.path)\n",
    "        # specifying target and data\n",
    "        data_input = data.iloc[:,1:data.shape[1]]\n",
    "        data_target = data.iloc[:,0]\n",
    "\n",
    "        if not self.localization:\n",
    "            data_target.loc[data_target.iloc[:]!=0] = 1\n",
    "\n",
    "        if self.localization or self.collision:\n",
    "            data_input = data_input.loc[data_target.iloc[:]!=0, :]\n",
    "            data_target = data_target.loc[data_target.iloc[:]!=0]\n",
    "            data_target = data_target-1\n",
    "\n",
    "\n",
    "        self.data_input = data_input.reset_index(drop=True)\n",
    "        self.data_target = data_target.reset_index(drop=True)\n",
    "        \n",
    "\n",
    "    def data_in_seq(self):\n",
    "\n",
    "        dof = self.dof\n",
    "\n",
    "        # resorting item position\n",
    "        data = np.array( range(0, self.num_features_dataset * self.data_seq ))\n",
    "        data = data.reshape(self.data_seq, self.num_features_dataset)\n",
    "\n",
    "        joint_data_pos = []\n",
    "        for j in range(dof):\n",
    "                 \n",
    "            column_index = np.array(range(self.num_features_lstm))*dof +j\n",
    "            row_index= range(self.data_seq-self.desired_seq, self.data_seq)\n",
    "            join_data_matrix = data[:, column_index]\n",
    "            joint_data_pos.append(join_data_matrix.reshape((len(column_index)*len(row_index))))\n",
    "        \n",
    "        joint_data_pos = np.hstack(joint_data_pos)\n",
    "\n",
    "        # resorting (28,28)---> (4,28)(4,28)(4,28)(4,28)(4,28)(4,28)(4,28)\n",
    "\n",
    "        self.data_input.columns = range(self.num_features_dataset * self.data_seq)\n",
    "        self.data_input = self.data_input.loc[:][joint_data_pos]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchmetrics import ConfusionMatrix, Accuracy\n",
    "\n",
    "class ZScoreNormalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ZScoreNormalization, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        std = torch.std(x, dim=1, keepdim=True)\n",
    "        return (x - mean) / (std + 1e-6)  # Adding epsilon to avoid division by zero\n",
    "\n",
    "class MinMaxNormalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MinMaxNormalization, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        min_val = torch.min(x, dim=1, keepdim=True)[0]\n",
    "        max_val = torch.max(x, dim=1, keepdim=True)[0]\n",
    "        return (x - min_val) / (max_val - min_val + 1e-6)  # Min-Max normalization with epsilon\n",
    "\n",
    "class Sequence(nn.Module):\n",
    "    def __init__(self, num_features_lstm=1, hidden_size=32, num_layers=3, time_seq=28, dropout=0.5, bidirectional=False):\n",
    "        super(Sequence, self).__init__()\n",
    "        self.normalization = ZScoreNormalization()\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features_lstm * time_seq,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0  # Apply dropout only if more than one layer\n",
    "        )\n",
    "        # Fully connected layer (to predict contact for each joint step)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_size*2, 1)  \n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 1) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Pass the input through the LSTM layer\n",
    "        normalized_input = self.normalization(input)\n",
    "        lstm_out, _ = self.lstm(normalized_input)\n",
    "        \n",
    "        # Pass each time step's output through the fully connected layer and sigmoid activation\n",
    "        # lstm_out shape: (batch_size, joint_seq, hidden_size)\n",
    "        joint_step_outputs = self.fc(lstm_out)  # Shape: (batch_size, joint_seq, 1)\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        #joint_step_outputs = self.sigmoid(joint_step_outputs)  # Shape: (batch_size, joint_seq, 1)\n",
    "        \n",
    "        return joint_step_outputs.squeeze()  # Return the prediction for each joint step (batch_size, joint_seq)\n",
    "\n",
    "\n",
    "def get_output(data_loader, model, device):\n",
    "    model.eval()\n",
    "    labels_pred = []\n",
    "    labels_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            output = model(X_batch) # Return the prediction for each joint step (batch_size, joint_seq)\n",
    "\n",
    "            preds = output.argmax(axis=1)  # shape: [batch_size]\n",
    "\n",
    "            labels_pred.append(preds.cpu().numpy())\n",
    "            labels_true.append(y_batch.cpu().numpy())\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    labels_pred = np.concatenate(labels_pred, axis=0)\n",
    "    labels_true = np.concatenate(labels_true, axis=0)\n",
    "\n",
    "    # Convert numpy arrays back to tensors\n",
    "    labels_pred = torch.tensor(labels_pred, dtype=torch.int64)  # Ensure long tensor for labels\n",
    "    labels_true = torch.tensor(labels_true, dtype=torch.int64)  # Ensure long tensor for labels\n",
    "\n",
    "    return labels_pred, labels_true\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Quadro T2000\n",
      "Feature batch shape: torch.Size([4096, 7, 28])\n",
      "Labels batch shape: torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "# Path to save trained models\n",
    "main_path = os.getcwd().replace('AIModels','')\n",
    "path_name = main_path + 'AIModels/trainedModels/'\n",
    "\n",
    "# Create directory if it does not exist\n",
    "if not os.path.exists(path_name):\n",
    "    os.makedirs(path_name)\n",
    "\n",
    "# Model configuration\n",
    "num_features_lstm = 1\n",
    "train_all_data = False  # Train a model using all available data\n",
    "\n",
    "collision = False\n",
    "localization = True\n",
    "\n",
    "batch_size = 4096\n",
    "num_classes = 7\n",
    "\n",
    "\n",
    "torch.manual_seed(2020)\n",
    "np.random.seed(2020)\n",
    "random.seed(2020)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name())\n",
    "\n",
    "# Load data and create training and testing sets\n",
    "training_data = create_tensor_dataset_localization(\n",
    "    main_path + '/dataset/localization/4dataset_train.pkl',\n",
    "    num_features_dataset=7, num_features_lstm=num_features_lstm, num_classes=num_classes,\n",
    "    collision=collision, localization=localization\n",
    ")\n",
    "testing_data = create_tensor_dataset_localization(\n",
    "    main_path + '/dataset/localization/4dataset_test.pkl',\n",
    "    num_features_dataset=7, num_features_lstm=num_features_lstm, num_classes=num_classes,\n",
    "    collision=collision, localization=localization\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "# Build the model\n",
    "model = Sequence(num_features_lstm=num_features_lstm, num_layers=1, hidden_size=64, dropout=0.1, bidirectional=True)\n",
    "model = model.double().to(device)  # Move model to device\n",
    "\n",
    "lr = 0.1\n",
    "# Use Adam optimizer and CrossEntropyLoss as the loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 - learning rate: 0.10000, classification loss: 0.3626\n",
      "Epoch: 2/50 - learning rate: 0.10000, classification loss: 0.3526\n",
      "Epoch: 3/50 - learning rate: 0.10000, classification loss: 0.3416\n",
      "Epoch: 4/50 - learning rate: 0.10000, classification loss: 0.3303\n",
      "Epoch: 5/50 - learning rate: 0.10000, classification loss: 0.3183\n",
      "Epoch: 6/50 - learning rate: 0.10000, classification loss: 0.3144\n",
      "Epoch: 7/50 - learning rate: 0.10000, classification loss: 0.3023\n",
      "Epoch: 8/50 - learning rate: 0.10000, classification loss: 0.3041\n",
      "Epoch: 9/50 - learning rate: 0.10000, classification loss: 0.3053\n",
      "Epoch    29: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Epoch: 10/50 - learning rate: 0.05000, classification loss: 0.2708\n",
      "Epoch: 11/50 - learning rate: 0.05000, classification loss: 0.2403\n",
      "Epoch: 12/50 - learning rate: 0.05000, classification loss: 0.2201\n",
      "Epoch: 13/50 - learning rate: 0.05000, classification loss: 0.2058\n",
      "Epoch: 14/50 - learning rate: 0.05000, classification loss: 0.1982\n",
      "Epoch: 15/50 - learning rate: 0.05000, classification loss: 0.1919\n",
      "Epoch: 16/50 - learning rate: 0.05000, classification loss: 0.1821\n",
      "Epoch: 17/50 - learning rate: 0.05000, classification loss: 0.1743\n",
      "Epoch: 18/50 - learning rate: 0.05000, classification loss: 0.1656\n",
      "Epoch: 19/50 - learning rate: 0.05000, classification loss: 0.1626\n",
      "Epoch: 20/50 - learning rate: 0.05000, classification loss: 0.1596\n",
      "Epoch: 21/50 - learning rate: 0.05000, classification loss: 0.1525\n",
      "Epoch: 22/50 - learning rate: 0.05000, classification loss: 0.1494\n",
      "Epoch: 23/50 - learning rate: 0.05000, classification loss: 0.1471\n",
      "Epoch: 24/50 - learning rate: 0.05000, classification loss: 0.1484\n",
      "Epoch: 25/50 - learning rate: 0.05000, classification loss: 0.1452\n",
      "Epoch: 26/50 - learning rate: 0.05000, classification loss: 0.1429\n",
      "Epoch: 27/50 - learning rate: 0.05000, classification loss: 0.1415\n",
      "Epoch: 28/50 - learning rate: 0.05000, classification loss: 0.1361\n",
      "Epoch: 29/50 - learning rate: 0.05000, classification loss: 0.1326\n",
      "Epoch: 30/50 - learning rate: 0.05000, classification loss: 0.1310\n",
      "Epoch: 31/50 - learning rate: 0.05000, classification loss: 0.1295\n",
      "Epoch: 32/50 - learning rate: 0.05000, classification loss: 0.1301\n",
      "Epoch: 33/50 - learning rate: 0.05000, classification loss: 0.1241\n",
      "Epoch: 34/50 - learning rate: 0.05000, classification loss: 0.1237\n",
      "Epoch: 35/50 - learning rate: 0.05000, classification loss: 0.1269\n",
      "Epoch: 36/50 - learning rate: 0.05000, classification loss: 0.1235\n",
      "Epoch: 37/50 - learning rate: 0.05000, classification loss: 0.1265\n",
      "Epoch: 38/50 - learning rate: 0.05000, classification loss: 0.1332\n",
      "Epoch    58: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Epoch: 39/50 - learning rate: 0.02500, classification loss: 0.1183\n",
      "Epoch: 40/50 - learning rate: 0.02500, classification loss: 0.1026\n",
      "Epoch: 41/50 - learning rate: 0.02500, classification loss: 0.0954\n",
      "Epoch: 42/50 - learning rate: 0.02500, classification loss: 0.0904\n",
      "Epoch: 43/50 - learning rate: 0.02500, classification loss: 0.0867\n",
      "Epoch: 44/50 - learning rate: 0.02500, classification loss: 0.0837\n",
      "Epoch: 45/50 - learning rate: 0.02500, classification loss: 0.0819\n",
      "Epoch: 46/50 - learning rate: 0.02500, classification loss: 0.0797\n",
      "Epoch: 47/50 - learning rate: 0.02500, classification loss: 0.0789\n",
      "Epoch: 48/50 - learning rate: 0.02500, classification loss: 0.0790\n",
      "Epoch: 49/50 - learning rate: 0.02500, classification loss: 0.0773\n",
      "Epoch: 50/50 - learning rate: 0.02500, classification loss: 0.0768\n",
      "On the test set:\n",
      " tensor([[ 290,   24,   13,    7,    6,   15,    3],\n",
      "        [  23,  966,   50,   32,   21,   27,   11],\n",
      "        [   3,   48,  782,   42,   25,   22,    7],\n",
      "        [   3,   34,   47, 1227,   64,   64,   26],\n",
      "        [   4,   13,   24,   72, 2156,  137,   74],\n",
      "        [   1,   18,    9,   59,  100, 2464,  102],\n",
      "        [   0,    5,    2,   19,   79,  149, 1257]])\n",
      "Accuray on the test set:\n",
      " tensor(0.8603)\n",
      "On the test set:\n",
      " tensor([[ 320,    9,    7,    6,    2,    8,    5],\n",
      "        [  15, 1037,   15,   12,   15,    9,    5],\n",
      "        [   4,   31,  897,   25,   11,    7,    4],\n",
      "        [   0,   17,   26, 1353,   27,   35,   12],\n",
      "        [   1,    7,   15,   27, 2385,   74,   37],\n",
      "        [   1,    6,    6,   26,   37, 2654,   45],\n",
      "        [   0,    5,    2,    5,   34,   59, 1481]])\n",
      "Accuray on the test set:\n",
      " tensor(0.9359)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "lr_threshold = 0.0005\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = []\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move data to device\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch.long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "    if train_all_data:\n",
    "        for X_batch, y_batch in test_dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move data to device\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "    avg_loss = np.mean(running_loss)\n",
    "    print(f\"Epoch: {epoch + 1}/{n_epochs} - learning rate: {optimizer.param_groups[0]['lr']:.5f}, classification loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Update the scheduler with the average loss\n",
    "    scheduler.step(avg_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if current_lr < lr_threshold:\n",
    "        print(f\"Learning rate has dropped below the threshold of {lr_threshold}. Stopping training.\")\n",
    "        break\n",
    "# Validation\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    confusionMatrix = ConfusionMatrix(task=\"multiclass\", num_classes=num_classes)\n",
    "    accuracy_metric = Accuracy()\n",
    "\n",
    "    # Update the metric with predictions and true labels\n",
    "    \n",
    "    y_pred, y_test = get_output(test_dataloader, model, device)\n",
    "    print(\"On the test set:\\n\", confusionMatrix(y_pred,y_test))\n",
    "    accuracy_metric.update(y_pred, y_test)\n",
    "    print(\"Accuray on the test set:\\n\", accuracy_metric.compute())\n",
    "    \n",
    "\n",
    "    #y_pred, y_train = get_output(training_data, model)\n",
    "    #print(\"On the train set:\\n\", confusionMatrix(y_train, y_pred))\n",
    "'''\n",
    "# Save the trained model\n",
    "named_tuple = time.localtime()\n",
    "if input('Do you want to save the data in trained models? (y/n): ') == 'y':\n",
    "    try:\n",
    "        if collision:\n",
    "            path_name_1 = path_name + '/collisionDetection/trainedModel' + str(time.strftime(\"_%m_%d_%Y_%H:%M:%S\", named_tuple)) + '.pth'\n",
    "            path_name_2 = path_name + '/collisionDetection/trainedModel.pth'\n",
    "        elif localization:\n",
    "            path_name_1 = path_name + '/localization/trainedModel' + str(time.strftime(\"_%m_%d_%Y_%H:%M:%S\", named_tuple)) + '.pth'\n",
    "            path_name_2 = path_name + '/localization/trainedModel.pth'\n",
    "        elif num_classes == 2:\n",
    "            path_name_1 = path_name + '/contactDetection/trainedModel' + str(time.strftime(\"_%m_%d_%Y_%H:%M:%S\", named_tuple)) + '.pth'\n",
    "            path_name_2 = path_name + '/contactDetection/trainedModel.pth'\n",
    "\n",
    "        torch.save({\"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"collision\": collision, \"localization\": localization, \"network_type\": network_type,\n",
    "                    \"n_epochs\": n_epochs, \"batch_size\": batch_size, \"num_features_lstm\": num_features_lstm,\n",
    "                    \"num_classes\": num_classes, \"lr\": lr}, path_name_1)\n",
    "        \n",
    "        torch.save({\"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"collision\": collision, \"localization\": localization, \"network_type\": network_type,\n",
    "                    \"n_epochs\": n_epochs, \"batch_size\": batch_size, \"num_features_lstm\": num_features_lstm,\n",
    "                    \"num_classes\": num_classes, \"lr\": lr}, path_name_2)\n",
    "        print('Model saved successfully!')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the model: {e}\")\n",
    "'''\n",
    "testing_data = create_tensor_dataset_localization( main_path + '/dataset/localization/0dataset_test.pkl',\n",
    "    num_features_dataset=7, num_features_lstm=1, num_classes=7,\n",
    "    collision=0, localization=1)\n",
    "    \n",
    "test_dataloader2 = DataLoader(testing_data, batch_size=batch_size, shuffle=True)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    confusionMatrix = ConfusionMatrix(task=\"multiclass\", num_classes=7)\n",
    "    accuracy_metric = Accuracy()\n",
    "\n",
    "    # Update the metric with predictions and true labels\n",
    "    \n",
    "    y_pred, y_test = get_output(test_dataloader2, model, device)\n",
    "    print(\"On the test set:\\n\", confusionMatrix(y_pred,y_test))\n",
    "    accuracy_metric.update(y_pred, y_test)\n",
    "    print(\"Accuray on the test set:\\n\", accuracy_metric.compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the test set:\n",
      " tensor([[ 317,   17,    5,    6,    3,    7,    2],\n",
      "        [  14, 1030,   22,   19,   11,    7,    5],\n",
      "        [   2,   33,  898,   15,   20,    8,    3],\n",
      "        [   1,   21,   18, 1360,   32,   25,   13],\n",
      "        [   2,    3,   12,   34, 2398,   66,   31],\n",
      "        [   0,    7,    3,   29,   31, 2673,   32],\n",
      "        [   0,    1,    3,   10,   47,   63, 1462]])\n",
      "Accuray on the test set:\n",
      " tensor(0.9369)\n"
     ]
    }
   ],
   "source": [
    "testing_data = create_tensor_dataset_localization( main_path + '/dataset/localization/0dataset_test.pkl',\n",
    "    num_features_dataset=7, num_features_lstm=1, num_classes=7,\n",
    "    collision=0, localization=1)\n",
    "    \n",
    "test_dataloader2 = DataLoader(testing_data, batch_size=batch_size, shuffle=True)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    confusionMatrix = ConfusionMatrix(task=\"multiclass\", num_classes=7)\n",
    "    accuracy_metric = Accuracy()\n",
    "\n",
    "    # Update the metric with predictions and true labels\n",
    "    \n",
    "    y_pred, y_test = get_output(test_dataloader2, model, device)\n",
    "    print(\"On the test set:\\n\", confusionMatrix(y_pred,y_test))\n",
    "    accuracy_metric.update(y_pred, y_test)\n",
    "    print(\"Accuray on the test set:\\n\", accuracy_metric.compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.05\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict()},'model_9344_110epoches.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save trained models\n",
    "model_path= 'model.pth'\n",
    "main_path = os.getcwd().replace('AIModels','')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size= 1024\n",
    "# Build the model\n",
    "model = Sequence(num_features_lstm=1, num_layers=1, hidden_size=64, dropout=0.1)\n",
    "model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
    "\n",
    "model = model.double().to(device)  # Move model to device\n",
    "model.eval()\n",
    "\n",
    "\n",
    "testing_data = create_tensor_dataset_localization( main_path + '/dataset/localization/0dataset_test.pkl',\n",
    "    num_features_dataset=7, num_features_lstm=1, num_classes=7,\n",
    "    collision=0, localization=1)\n",
    "    \n",
    "test_dataloader2 = DataLoader(testing_data, batch_size=batch_size, shuffle=True)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    confusionMatrix = ConfusionMatrix(task=\"multiclass\", num_classes=7)\n",
    "    accuracy_metric = Accuracy()\n",
    "\n",
    "    # Update the metric with predictions and true labels\n",
    "    \n",
    "    y_pred, y_test = get_output(test_dataloader2, model, device)\n",
    "    print(\"On the test set:\\n\", confusionMatrix(y_pred,y_test))\n",
    "    accuracy_metric.update(y_pred, y_test)\n",
    "    print(\"Accuray on the test set:\\n\", accuracy_metric.compute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing on second franka robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set</th>\n",
       "      <th>test_on_i_dataset</th>\n",
       "      <th>test_on_0_dataset</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>81.31</td>\n",
       "      <td>81.31</td>\n",
       "      <td>235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>79.77</td>\n",
       "      <td>83.41</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>80.25</td>\n",
       "      <td>83.78</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>80.93</td>\n",
       "      <td>83.12</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>81.97</td>\n",
       "      <td>84.44</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Set  test_on_i_dataset  test_on_0_dataset   Time\n",
       "0    0              81.31              81.31  235.0\n",
       "1    1              79.77              83.41  117.0\n",
       "2    2              80.25              83.78  155.0\n",
       "3    3              80.93              83.12  155.0\n",
       "4    4              81.97              84.44  101.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing on UR robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "franka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
